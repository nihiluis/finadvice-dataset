{"cells":[{"cell_type":"markdown","metadata":{},"source":["This approach is currently not implemented by default and can improve generation quality. Source: https://twitter.com/Tim_Dettmers/status/1695352747694919931"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-27T18:35:57.756395Z","iopub.status.busy":"2023-08-27T18:35:57.755778Z","iopub.status.idle":"2023-08-27T18:36:53.780658Z","shell.execute_reply":"2023-08-27T18:36:53.779390Z","shell.execute_reply.started":"2023-08-27T18:35:57.756360Z"},"trusted":true},"outputs":[],"source":["!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n","!pip install datasets bitsandbytes einops -Uqqq"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-27T18:37:20.688253Z","iopub.status.busy":"2023-08-27T18:37:20.687892Z","iopub.status.idle":"2023-08-27T18:37:20.696345Z","shell.execute_reply":"2023-08-27T18:37:20.693126Z","shell.execute_reply.started":"2023-08-27T18:37:20.688222Z"},"trusted":true},"outputs":[],"source":["import torch\n","import peft\n","import json\n","import shutil\n","from peft.utils import _get_submodules\n","import os\n","import bitsandbytes as bnb\n","from bitsandbytes.functional import dequantize_4bit\n","from peft import PeftModel\n","from transformers import AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n","import gc\n","import copy"]},{"cell_type":"markdown","metadata":{},"source":["Taken from: https://gist.github.com/ChrisHayduk/1a53463331f52dca205e55982baf9930"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-27T19:07:11.762790Z","iopub.status.busy":"2023-08-27T19:07:11.762380Z","iopub.status.idle":"2023-08-27T19:07:11.774770Z","shell.execute_reply":"2023-08-27T19:07:11.773552Z","shell.execute_reply.started":"2023-08-27T19:07:11.762755Z"},"trusted":true},"outputs":[],"source":["def dequantize_model(model, tokenizer, save_to='./dequantized_model', dtype=torch.float16, device=\"cuda\"):\n","    \"\"\"\n","    'model': the peftmodel you loaded with qlora.\n","    'tokenizer': the model's corresponding hf's tokenizer.\n","    'to': directory to save the dequantized model\n","    'dtype': dtype that the model was trained using\n","    'device': device to load the model to\n","    \"\"\"\n","\n","    if save_to != None:\n","        # Delete the model object if it exists\n","        if os.path.exists(save_to):\n","            shutil.rmtree(save_to)\n","\n","        os.makedirs(to, exist_ok=True)\n","\n","    cls = bnb.nn.Linear4bit\n","\n","    with torch.no_grad():\n","        for name, module in model.named_modules():\n","            if isinstance(module, cls):\n","                print(f\"Dequantizing `{name}`...\")\n","                quant_state = copy.deepcopy(module.weight.quant_state)\n","\n","                quant_state[2] = dtype\n","\n","                weights = dequantize_4bit(module.weight.data, quant_state=quant_state, quant_type=\"nf4\").to(dtype)\n","\n","                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=None, dtype=dtype)\n","                new_module.weight = torch.nn.Parameter(weights)\n","                new_module.to(device=device, dtype=dtype)\n","\n","                parent, target, target_name = _get_submodules(model, name)\n","                setattr(parent, target_name, new_module)\n","\n","        # a hack, setting this to avoid hf's saving error because hf\n","        # itself does not support saving a model that is registered to be loaded in 4bit.\n","        model.is_loaded_in_4bit = False\n","\n","        if save_to != None:\n","            print(\"Saving dequantized model...\")\n","            model.save_pretrained(save_to)\n","            tokenizer.save_pretrained(save_to)\n","            config_data = json.loads(open(os.path.join(save_to, 'config.json'), 'r').read())\n","            config_data.pop(\"quantization_config\", None)\n","            config_data.pop(\"pretraining_tp\", None)\n","            with open(os.path.join(to, 'config.json'), 'w') as config:\n","                config.write(json.dumps(config_data, indent=2))\n","        \n","        return model"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-27T19:07:15.856088Z","iopub.status.busy":"2023-08-27T19:07:15.855730Z","iopub.status.idle":"2023-08-27T19:14:38.138039Z","shell.execute_reply":"2023-08-27T19:14:38.136958Z","shell.execute_reply.started":"2023-08-27T19:07:15.856056Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting to load the model abhishek/llama-2-7b-hf-small-shards into memory\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ca0108ca3a049279b147bdcaa427397","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cbcf678605d24605b5a250d6bcfa589d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"959909513242439887477aba4477d53a","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36b44b26b7ca4c6f90acc2c815fda8b0","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00001-of-00010.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a4dfb3ff15f47508eb671362bfba9cc","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00002-of-00010.bin:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66bd4e4877dc4878bb279d7a27e240b3","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00003-of-00010.bin:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06f07cee283643dfbb1f7ec83f887e23","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00004-of-00010.bin:   0%|          | 0.00/2.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfe63c55d3114cddba9bf96fd2042c1c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00005-of-00010.bin:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8c3432f282a453287e0c2336af55133","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00006-of-00010.bin:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"432a4207017744e295732cd9587928f2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00007-of-00010.bin:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42530c9f63b74d378d35294adcb5ae2a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00008-of-00010.bin:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8c5ab87146f4fe9a56aa840e8e6a7f3","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00009-of-00010.bin:   0%|          | 0.00/2.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d3c3aecf72646d792605a05b5f3702c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00010-of-00010.bin:   0%|          | 0.00/705M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1146f6015dac4b738262ee9ae6b03352","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ae32e2eef524c47986d8e48104317ac","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0ffd404b7fc49aca2036b49533540fe","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df93c390071f4a0297888c4d79b28d47","version_major":2,"version_minor":0},"text/plain":["Downloading (…)in/added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb4cc562fe0c47e289214c2f7d1e4148","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"541fcbc379424cfcbd654f6f50bf0ec9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dequantizing `model.layers.0.self_attn.q_proj`...\n","Dequantizing `model.layers.0.self_attn.k_proj`...\n","Dequantizing `model.layers.0.self_attn.v_proj`...\n","Dequantizing `model.layers.0.self_attn.o_proj`...\n","Dequantizing `model.layers.0.mlp.gate_proj`...\n","Dequantizing `model.layers.0.mlp.up_proj`...\n","Dequantizing `model.layers.0.mlp.down_proj`...\n","Dequantizing `model.layers.1.self_attn.q_proj`...\n","Dequantizing `model.layers.1.self_attn.k_proj`...\n","Dequantizing `model.layers.1.self_attn.v_proj`...\n","Dequantizing `model.layers.1.self_attn.o_proj`...\n","Dequantizing `model.layers.1.mlp.gate_proj`...\n","Dequantizing `model.layers.1.mlp.up_proj`...\n","Dequantizing `model.layers.1.mlp.down_proj`...\n","Dequantizing `model.layers.2.self_attn.q_proj`...\n","Dequantizing `model.layers.2.self_attn.k_proj`...\n","Dequantizing `model.layers.2.self_attn.v_proj`...\n","Dequantizing `model.layers.2.self_attn.o_proj`...\n","Dequantizing `model.layers.2.mlp.gate_proj`...\n","Dequantizing `model.layers.2.mlp.up_proj`...\n","Dequantizing `model.layers.2.mlp.down_proj`...\n","Dequantizing `model.layers.3.self_attn.q_proj`...\n","Dequantizing `model.layers.3.self_attn.k_proj`...\n","Dequantizing `model.layers.3.self_attn.v_proj`...\n","Dequantizing `model.layers.3.self_attn.o_proj`...\n","Dequantizing `model.layers.3.mlp.gate_proj`...\n","Dequantizing `model.layers.3.mlp.up_proj`...\n","Dequantizing `model.layers.3.mlp.down_proj`...\n","Dequantizing `model.layers.4.self_attn.q_proj`...\n","Dequantizing `model.layers.4.self_attn.k_proj`...\n","Dequantizing `model.layers.4.self_attn.v_proj`...\n","Dequantizing `model.layers.4.self_attn.o_proj`...\n","Dequantizing `model.layers.4.mlp.gate_proj`...\n","Dequantizing `model.layers.4.mlp.up_proj`...\n","Dequantizing `model.layers.4.mlp.down_proj`...\n","Dequantizing `model.layers.5.self_attn.q_proj`...\n","Dequantizing `model.layers.5.self_attn.k_proj`...\n","Dequantizing `model.layers.5.self_attn.v_proj`...\n","Dequantizing `model.layers.5.self_attn.o_proj`...\n","Dequantizing `model.layers.5.mlp.gate_proj`...\n","Dequantizing `model.layers.5.mlp.up_proj`...\n","Dequantizing `model.layers.5.mlp.down_proj`...\n","Dequantizing `model.layers.6.self_attn.q_proj`...\n","Dequantizing `model.layers.6.self_attn.k_proj`...\n","Dequantizing `model.layers.6.self_attn.v_proj`...\n","Dequantizing `model.layers.6.self_attn.o_proj`...\n","Dequantizing `model.layers.6.mlp.gate_proj`...\n","Dequantizing `model.layers.6.mlp.up_proj`...\n","Dequantizing `model.layers.6.mlp.down_proj`...\n","Dequantizing `model.layers.7.self_attn.q_proj`...\n","Dequantizing `model.layers.7.self_attn.k_proj`...\n","Dequantizing `model.layers.7.self_attn.v_proj`...\n","Dequantizing `model.layers.7.self_attn.o_proj`...\n","Dequantizing `model.layers.7.mlp.gate_proj`...\n","Dequantizing `model.layers.7.mlp.up_proj`...\n","Dequantizing `model.layers.7.mlp.down_proj`...\n","Dequantizing `model.layers.8.self_attn.q_proj`...\n","Dequantizing `model.layers.8.self_attn.k_proj`...\n","Dequantizing `model.layers.8.self_attn.v_proj`...\n","Dequantizing `model.layers.8.self_attn.o_proj`...\n","Dequantizing `model.layers.8.mlp.gate_proj`...\n","Dequantizing `model.layers.8.mlp.up_proj`...\n","Dequantizing `model.layers.8.mlp.down_proj`...\n","Dequantizing `model.layers.9.self_attn.q_proj`...\n","Dequantizing `model.layers.9.self_attn.k_proj`...\n","Dequantizing `model.layers.9.self_attn.v_proj`...\n","Dequantizing `model.layers.9.self_attn.o_proj`...\n","Dequantizing `model.layers.9.mlp.gate_proj`...\n","Dequantizing `model.layers.9.mlp.up_proj`...\n","Dequantizing `model.layers.9.mlp.down_proj`...\n","Dequantizing `model.layers.10.self_attn.q_proj`...\n","Dequantizing `model.layers.10.self_attn.k_proj`...\n","Dequantizing `model.layers.10.self_attn.v_proj`...\n","Dequantizing `model.layers.10.self_attn.o_proj`...\n","Dequantizing `model.layers.10.mlp.gate_proj`...\n","Dequantizing `model.layers.10.mlp.up_proj`...\n","Dequantizing `model.layers.10.mlp.down_proj`...\n","Dequantizing `model.layers.11.self_attn.q_proj`...\n","Dequantizing `model.layers.11.self_attn.k_proj`...\n","Dequantizing `model.layers.11.self_attn.v_proj`...\n","Dequantizing `model.layers.11.self_attn.o_proj`...\n","Dequantizing `model.layers.11.mlp.gate_proj`...\n","Dequantizing `model.layers.11.mlp.up_proj`...\n","Dequantizing `model.layers.11.mlp.down_proj`...\n","Dequantizing `model.layers.12.self_attn.q_proj`...\n","Dequantizing `model.layers.12.self_attn.k_proj`...\n","Dequantizing `model.layers.12.self_attn.v_proj`...\n","Dequantizing `model.layers.12.self_attn.o_proj`...\n","Dequantizing `model.layers.12.mlp.gate_proj`...\n","Dequantizing `model.layers.12.mlp.up_proj`...\n","Dequantizing `model.layers.12.mlp.down_proj`...\n","Dequantizing `model.layers.13.self_attn.q_proj`...\n","Dequantizing `model.layers.13.self_attn.k_proj`...\n","Dequantizing `model.layers.13.self_attn.v_proj`...\n","Dequantizing `model.layers.13.self_attn.o_proj`...\n","Dequantizing `model.layers.13.mlp.gate_proj`...\n","Dequantizing `model.layers.13.mlp.up_proj`...\n","Dequantizing `model.layers.13.mlp.down_proj`...\n","Dequantizing `model.layers.14.self_attn.q_proj`...\n","Dequantizing `model.layers.14.self_attn.k_proj`...\n","Dequantizing `model.layers.14.self_attn.v_proj`...\n","Dequantizing `model.layers.14.self_attn.o_proj`...\n","Dequantizing `model.layers.14.mlp.gate_proj`...\n","Dequantizing `model.layers.14.mlp.up_proj`...\n","Dequantizing `model.layers.14.mlp.down_proj`...\n","Dequantizing `model.layers.15.self_attn.q_proj`...\n","Dequantizing `model.layers.15.self_attn.k_proj`...\n","Dequantizing `model.layers.15.self_attn.v_proj`...\n","Dequantizing `model.layers.15.self_attn.o_proj`...\n","Dequantizing `model.layers.15.mlp.gate_proj`...\n","Dequantizing `model.layers.15.mlp.up_proj`...\n","Dequantizing `model.layers.15.mlp.down_proj`...\n","Dequantizing `model.layers.16.self_attn.q_proj`...\n","Dequantizing `model.layers.16.self_attn.k_proj`...\n","Dequantizing `model.layers.16.self_attn.v_proj`...\n","Dequantizing `model.layers.16.self_attn.o_proj`...\n","Dequantizing `model.layers.16.mlp.gate_proj`...\n","Dequantizing `model.layers.16.mlp.up_proj`...\n","Dequantizing `model.layers.16.mlp.down_proj`...\n","Dequantizing `model.layers.17.self_attn.q_proj`...\n","Dequantizing `model.layers.17.self_attn.k_proj`...\n","Dequantizing `model.layers.17.self_attn.v_proj`...\n","Dequantizing `model.layers.17.self_attn.o_proj`...\n","Dequantizing `model.layers.17.mlp.gate_proj`...\n","Dequantizing `model.layers.17.mlp.up_proj`...\n","Dequantizing `model.layers.17.mlp.down_proj`...\n","Dequantizing `model.layers.18.self_attn.q_proj`...\n","Dequantizing `model.layers.18.self_attn.k_proj`...\n","Dequantizing `model.layers.18.self_attn.v_proj`...\n","Dequantizing `model.layers.18.self_attn.o_proj`...\n","Dequantizing `model.layers.18.mlp.gate_proj`...\n","Dequantizing `model.layers.18.mlp.up_proj`...\n","Dequantizing `model.layers.18.mlp.down_proj`...\n","Dequantizing `model.layers.19.self_attn.q_proj`...\n","Dequantizing `model.layers.19.self_attn.k_proj`...\n","Dequantizing `model.layers.19.self_attn.v_proj`...\n","Dequantizing `model.layers.19.self_attn.o_proj`...\n","Dequantizing `model.layers.19.mlp.gate_proj`...\n","Dequantizing `model.layers.19.mlp.up_proj`...\n","Dequantizing `model.layers.19.mlp.down_proj`...\n","Dequantizing `model.layers.20.self_attn.q_proj`...\n","Dequantizing `model.layers.20.self_attn.k_proj`...\n","Dequantizing `model.layers.20.self_attn.v_proj`...\n","Dequantizing `model.layers.20.self_attn.o_proj`...\n","Dequantizing `model.layers.20.mlp.gate_proj`...\n","Dequantizing `model.layers.20.mlp.up_proj`...\n","Dequantizing `model.layers.20.mlp.down_proj`...\n","Dequantizing `model.layers.21.self_attn.q_proj`...\n","Dequantizing `model.layers.21.self_attn.k_proj`...\n","Dequantizing `model.layers.21.self_attn.v_proj`...\n","Dequantizing `model.layers.21.self_attn.o_proj`...\n","Dequantizing `model.layers.21.mlp.gate_proj`...\n","Dequantizing `model.layers.21.mlp.up_proj`...\n","Dequantizing `model.layers.21.mlp.down_proj`...\n","Dequantizing `model.layers.22.self_attn.q_proj`...\n","Dequantizing `model.layers.22.self_attn.k_proj`...\n","Dequantizing `model.layers.22.self_attn.v_proj`...\n","Dequantizing `model.layers.22.self_attn.o_proj`...\n","Dequantizing `model.layers.22.mlp.gate_proj`...\n","Dequantizing `model.layers.22.mlp.up_proj`...\n","Dequantizing `model.layers.22.mlp.down_proj`...\n","Dequantizing `model.layers.23.self_attn.q_proj`...\n","Dequantizing `model.layers.23.self_attn.k_proj`...\n","Dequantizing `model.layers.23.self_attn.v_proj`...\n","Dequantizing `model.layers.23.self_attn.o_proj`...\n","Dequantizing `model.layers.23.mlp.gate_proj`...\n","Dequantizing `model.layers.23.mlp.up_proj`...\n","Dequantizing `model.layers.23.mlp.down_proj`...\n","Dequantizing `model.layers.24.self_attn.q_proj`...\n","Dequantizing `model.layers.24.self_attn.k_proj`...\n","Dequantizing `model.layers.24.self_attn.v_proj`...\n","Dequantizing `model.layers.24.self_attn.o_proj`...\n","Dequantizing `model.layers.24.mlp.gate_proj`...\n","Dequantizing `model.layers.24.mlp.up_proj`...\n","Dequantizing `model.layers.24.mlp.down_proj`...\n","Dequantizing `model.layers.25.self_attn.q_proj`...\n","Dequantizing `model.layers.25.self_attn.k_proj`...\n","Dequantizing `model.layers.25.self_attn.v_proj`...\n","Dequantizing `model.layers.25.self_attn.o_proj`...\n","Dequantizing `model.layers.25.mlp.gate_proj`...\n","Dequantizing `model.layers.25.mlp.up_proj`...\n","Dequantizing `model.layers.25.mlp.down_proj`...\n","Dequantizing `model.layers.26.self_attn.q_proj`...\n","Dequantizing `model.layers.26.self_attn.k_proj`...\n","Dequantizing `model.layers.26.self_attn.v_proj`...\n","Dequantizing `model.layers.26.self_attn.o_proj`...\n","Dequantizing `model.layers.26.mlp.gate_proj`...\n","Dequantizing `model.layers.26.mlp.up_proj`...\n","Dequantizing `model.layers.26.mlp.down_proj`...\n","Dequantizing `model.layers.27.self_attn.q_proj`...\n","Dequantizing `model.layers.27.self_attn.k_proj`...\n","Dequantizing `model.layers.27.self_attn.v_proj`...\n","Dequantizing `model.layers.27.self_attn.o_proj`...\n","Dequantizing `model.layers.27.mlp.gate_proj`...\n","Dequantizing `model.layers.27.mlp.up_proj`...\n","Dequantizing `model.layers.27.mlp.down_proj`...\n","Dequantizing `model.layers.28.self_attn.q_proj`...\n","Dequantizing `model.layers.28.self_attn.k_proj`...\n","Dequantizing `model.layers.28.self_attn.v_proj`...\n","Dequantizing `model.layers.28.self_attn.o_proj`...\n","Dequantizing `model.layers.28.mlp.gate_proj`...\n","Dequantizing `model.layers.28.mlp.up_proj`...\n","Dequantizing `model.layers.28.mlp.down_proj`...\n","Dequantizing `model.layers.29.self_attn.q_proj`...\n","Dequantizing `model.layers.29.self_attn.k_proj`...\n","Dequantizing `model.layers.29.self_attn.v_proj`...\n","Dequantizing `model.layers.29.self_attn.o_proj`...\n","Dequantizing `model.layers.29.mlp.gate_proj`...\n","Dequantizing `model.layers.29.mlp.up_proj`...\n","Dequantizing `model.layers.29.mlp.down_proj`...\n","An error occurred: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 15.90 GiB total capacity; 14.88 GiB already allocated; 73.75 MiB free; 15.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","Model, GPU cache, and garbage have been cleared.\n"]}],"source":["model_path = \"abhishek/llama-2-7b-hf-small-shards\"\n","adapter_path = \"nihiluis/finadv100\"\n","\n","quantization_config=BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","        )\n","\n","try:\n","    print(f\"Starting to load the model {model_path} into memory\")\n","\n","    model = LlamaForCausalLM.from_pretrained(\n","        model_path,\n","        load_in_4bit=True,\n","        torch_dtype=torch.float16,\n","        quantization_config=quantization_config,\n","        device_map={\"\": 0}\n","    )\n","    print(model)\n","    tok = LlamaTokenizer.from_pretrained(model_path)\n","    model = dequantize_model(model, tok, save_to=None)\n","    print(model)\n","    model = PeftModel.from_pretrained(model = model, model_id = adapter_path)\n","    print(model)\n","    model = model.merge_and_unload()\n","    print(model)\n","    \n","    print(f\"Successfully loaded the model {model_path} into memory\")\n","\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","\n","    # Delete the model object if it exists\n","    if 'model' in locals():\n","        del model\n","\n","    # Clear the GPU cache\n","    torch.cuda.empty_cache()\n","\n","    # Run the garbage collection\n","    gc.collect()\n","\n","    print(\"Model, GPU cache, and garbage have been cleared.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
